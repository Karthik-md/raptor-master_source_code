{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912cd8c6-d405-4dfe-8897-46108e6a6af7",
   "metadata": {},
   "source": [
    "# RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631b09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: An OpenAI API key must be set here for application initialization, even if not in use.\n",
    "# If you're not utilizing OpenAI models, assign a placeholder string (e.g., \"not_used\").\n",
    "import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d51ebd-5597-4fdd-8c37-32636395081b",
   "metadata": {},
   "source": [
    "1) **Building**: RAPTOR recursively embeds, clusters, and summarizes chunks of text to construct a tree with varying levels of summarization from the bottom up. You can create a tree from the text in 'sample.txt' using `RA.add_documents(text)`.\n",
    "\n",
    "2) **Querying**: At inference time, the RAPTOR model retrieves information from this tree, integrating data across lengthy documents at different abstraction levels. You can perform queries on the tree with `RA.answer_question`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f58830-9004-48a4-b50e-61a855511d24",
   "metadata": {},
   "source": [
    "### Building the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3753fcf9-0a8e-4ab3-bf3a-6be38ef6cd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karth\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-05 16:24:18,050 - Loading faiss with AVX2 support.\n",
      "2024-07-05 16:24:18,631 - Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "from raptor import RetrievalAugmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ab6ea-1c79-4ed1-97de-1c2e39d6db2e",
   "metadata": {},
   "source": [
    "## Using other Open Source Models for Summarization/QA/Embeddings\n",
    "\n",
    "If you want to use other models such as Llama or Mistral, you can very easily define your own models and use them with RAPTOR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f86cbe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from raptor import BaseSummarizationModel, BaseQAModel, BaseEmbeddingModel, RetrievalAugmentationConfig\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe5cef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\karth\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "add_to_git_credential=True\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "245b91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class SummarizationModel(BaseSummarizationModel):\n",
    "    def __init__(self, url=\"http://a0221.nhr.fau.de:5000/v1/chat/completions\"):\n",
    "        super().__init__()  # Initialize from BaseSummarizationModel if needed\n",
    "        self.url = url\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        self.history = []\n",
    "\n",
    "    def summarize(self, context, max_tokens=150):\n",
    "        # Clear history for each new summarization request\n",
    "        self.history = []\n",
    "\n",
    "        # Construct the user message for summarization\n",
    "        user_message = f\"{context}\"\n",
    "        self.history.append({\"role\": \"user\", \"content\": f\"Write a summary of the following, including as many key details as possible: {context}:\"})\n",
    "\n",
    "        # Prepare the data payload\n",
    "        data = {\n",
    "            \"mode\": \"instruct\",\n",
    "            \"temperature\": 0.7,\n",
    "            \"messages\": self.history\n",
    "        }\n",
    "\n",
    "        # Make the POST request to the specified URL\n",
    "        try:\n",
    "            response = requests.post(self.url, headers=self.headers, json=data, verify=False)\n",
    "\n",
    "            # Check if the response is successful\n",
    "            if response.status_code == 200:\n",
    "                print(response.json())\n",
    "                assistant_message = response.json()['choices'][0]['message']['content']\n",
    "                print(assistant_message)\n",
    "                return assistant_message.strip()\n",
    "            else:\n",
    "                return f\"Error: {response.status_code} {response.text}\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Request error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a171496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class QAModel(BaseQAModel):\n",
    "    def __init__(self, url=\"http://a0221.nhr.fau.de:5000/v1/chat/completions\"):\n",
    "        super().__init__()  # Initialize from BaseSummarizationModel if needed\n",
    "        self.url = url\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        self.history = []\n",
    "\n",
    "    def answer_question(self, context, question):\n",
    "        # Clear history for each new summarization request\n",
    "        self.history = []\n",
    "\n",
    "        \n",
    "        \n",
    "        self.history.append({\"role\": \"user\", \"content\": f\"Given Context: {context} Give the best full answer amongst the option to question {question}\"})\n",
    "\n",
    "        # Prepare the data payload\n",
    "        data = {\n",
    "            \"mode\": \"instruct\",\n",
    "            \"temperature\": 0.7,\n",
    "            \"messages\": self.history\n",
    "        }\n",
    "\n",
    "        # Make the POST request to the specified URL\n",
    "        try:\n",
    "            response = requests.post(self.url, headers=self.headers, json=data, verify=False)\n",
    "\n",
    "            # Check if the response is successful\n",
    "            if response.status_code == 200:\n",
    "                print(response.json())\n",
    "                assistant_message = response.json()['choices'][0]['message']['content']\n",
    "                print(assistant_message)\n",
    "                return assistant_message.strip()\n",
    "            else:\n",
    "                return f\"Error: {response.status_code} {response.text}\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Request error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "878f7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "class SBertEmbeddingModel(BaseEmbeddingModel):\n",
    "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def create_embedding(self, text):\n",
    "        return self.model.encode(text,show_progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "255791ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 16:24:56,490 - Load pretrained SentenceTransformer: sentence-transformers/multi-qa-mpnet-base-cos-v1\n",
      "2024-07-05 16:24:57,857 - Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "RAC = RetrievalAugmentationConfig(summarization_model=SummarizationModel(), qa_model=QAModel(), embedding_model=SBertEmbeddingModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee46f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 16:25:03,463 - Successfully initialized TreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x00000217C71581C0>\n",
      "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x00000217C7158970>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2024-07-05 16:25:03,465 - Successfully initialized ClusterTreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x00000217C71581C0>\n",
      "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x00000217C7158970>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2024-07-05 16:25:03,468 - Successfully initialized RetrievalAugmentation with Config \n",
      "        RetrievalAugmentationConfig:\n",
      "            \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x00000217C71581C0>\n",
      "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x00000217C7158970>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "            \n",
      "            \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <__main__.SBertEmbeddingModel object at 0x00000217C7158970>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "            \n",
      "            QA Model: <__main__.GEMMAQAModel object at 0x00000217C7158A00>\n",
      "            Tree Builder Type: cluster\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "RA = RetrievalAugmentation(config=RAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afe05daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ", Module name 1750: Advanced Processes Basics I (Advanced Processes)5 ECTS\n",
      "Course / lectures: Vorle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 16:26:05,493 - Creating Leaf Nodes\n"
     ]
    }
   ],
   "source": [
    "with open('demo/tech_txt.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "print(text[:100])    \n",
    "RA.add_documents(text)\n",
    "\n",
    "SAVE_PATH = \"demo/tech_txt_tree_structure\"\n",
    "RA.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee5847",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Difference between Masters in computation Engineering and Masters in Artificial Intelligence in terms of credits?\"\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5131864",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = RA.tree\n",
    "tree.root_nodes\n",
    "def print_tree_layers(root_nodes):\n",
    "    \"\"\"\n",
    "    Iterates over the tree from the root nodes and prints node index and text layer by layer.\n",
    "\n",
    "    Args:\n",
    "      root_nodes: A dictionary mapping node index to Node objects.\n",
    "    \"\"\"\n",
    "        \n",
    "    all_nodes = tree.all_nodes\n",
    "    current_layer = list(root_nodes.values())  # Convert root_nodes to a list for iteration\n",
    "    level = 0\n",
    "    while current_layer:\n",
    "        print(f\"================= Level {level} ================= \")\n",
    "        next_layer = []\n",
    "        for node in current_layer:\n",
    "            print(f\"Index: {node.index}, Text: {node.text}\\n\")\n",
    "            next_layer.extend(all_nodes.get(child_index) for child_index in node.children)\n",
    "        \n",
    "        current_layer = next_layer\n",
    "        level += 1\n",
    "\n",
    "print_tree_layers(tree.root_nodes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
