{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912cd8c6-d405-4dfe-8897-46108e6a6af7",
   "metadata": {},
   "source": [
    "# RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: An OpenAI API key must be set here for application initialization, even if not in use.\n",
    "# If you're not utilizing OpenAI models, assign a placeholder string (e.g., \"not_used\").\n",
    "import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d51ebd-5597-4fdd-8c37-32636395081b",
   "metadata": {},
   "source": [
    "1) **Building**: RAPTOR recursively embeds, clusters, and summarizes chunks of text to construct a tree with varying levels of summarization from the bottom up. You can create a tree from the text in 'sample.txt' using `RA.add_documents(text)`.\n",
    "\n",
    "2) **Querying**: At inference time, the RAPTOR model retrieves information from this tree, integrating data across lengthy documents at different abstraction levels. You can perform queries on the tree with `RA.answer_question`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f58830-9004-48a4-b50e-61a855511d24",
   "metadata": {},
   "source": [
    "### Building the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753fcf9-0a8e-4ab3-bf3a-6be38ef6cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor import RetrievalAugmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ab6ea-1c79-4ed1-97de-1c2e39d6db2e",
   "metadata": {},
   "source": [
    "## Using other Open Source Models for Summarization/QA/Embeddings\n",
    "\n",
    "If you want to use other models such as Llama or Mistral, you can very easily define your own models and use them with RAPTOR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cbe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from raptor import BaseSummarizationModel, BaseQAModel, BaseEmbeddingModel, RetrievalAugmentationConfig\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5cef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "add_to_git_credential=True\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c6e56",
   "metadata": {},
   "source": [
    "## Building Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class SummarizationModel(BaseSummarizationModel):\n",
    "    def __init__(self, url=\"http://a0221.nhr.fau.de:5000/v1/chat/completions\"):\n",
    "        super().__init__()  # Initialize from BaseSummarizationModel if needed\n",
    "        self.url = url\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        self.history = []\n",
    "\n",
    "    def summarize(self, context, max_tokens=150):\n",
    "        # Clear history for each new summarization request\n",
    "        self.history = []\n",
    "\n",
    "        # Construct the user message for summarization\n",
    "        user_message = f\"{context}\"\n",
    "        self.history.append({\"role\": \"user\", \"content\": f\"Write a summary of the following, including as many key details as possible: {context}:\"})\n",
    "\n",
    "        # Prepare the data payload\n",
    "        data = {\n",
    "            \"mode\": \"instruct\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"messages\": self.history\n",
    "        }\n",
    "\n",
    "        # Make the POST request to the specified URL\n",
    "        try:\n",
    "            response = requests.post(self.url, headers=self.headers, json=data, verify=False)\n",
    "\n",
    "            # Check if the response is successful\n",
    "            if response.status_code == 200:\n",
    "                print(response.json())\n",
    "                assistant_message = response.json()['choices'][0]['message']['content']\n",
    "                print(assistant_message)\n",
    "                return assistant_message.strip()\n",
    "            else:\n",
    "                return f\"Error: {response.status_code} {response.text}\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Request error: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb3950",
   "metadata": {},
   "source": [
    "## Building the QA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class QAModel(BaseQAModel):\n",
    "    def __init__(self, url=\"http://a0221.nhr.fau.de:5000/v1/chat/completions\"):\n",
    "        super().__init__()  # Initialize from BaseSummarizationModel if needed\n",
    "        self.url = url\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        self.history = []\n",
    "\n",
    "    def answer_question(self, context, question):\n",
    "        # Clear history for each new summarization request\n",
    "        self.history = []\n",
    "       \n",
    "        \n",
    "        summarized_content=SummarizationModel().summarize(context)\n",
    "        self.history.append({\"role\": \"user\", \"content\": f\"context: {summarized_content}\\n\\nQuestion: {question}\\n\\nPlease provide a detailed and informative answer based on context above.\"})\n",
    "\n",
    "        # Prepare the data payload\n",
    "        data = {\n",
    "            \"mode\": \"instruct\",\n",
    "            \"temperature\": 0.7,\n",
    "            \"messages\": self.history\n",
    "        }\n",
    "        \n",
    "        # Make the POST request to the specified URL\n",
    "        try:\n",
    "            response = requests.post(self.url, headers=self.headers, json=data, verify=False)\n",
    "\n",
    "            # Check if the response is successful\n",
    "            if response.status_code == 200:\n",
    "                print(response.json())\n",
    "                assistant_message = response.json()['choices'][0]['message']['content']\n",
    "                print(assistant_message)\n",
    "                return assistant_message.strip()\n",
    "            else:\n",
    "                return f\"Error: {response.status_code} {response.text}\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Request error: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d22bec5",
   "metadata": {},
   "source": [
    "## Building the embedding model by sentense transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "class SBertEmbeddingModel(BaseEmbeddingModel):\n",
    "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def create_embedding(self, text):\n",
    "        return self.model.encode(text,show_progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255791ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAC = RetrievalAugmentationConfig(summarization_model=SummarizationModel(), qa_model=QAModel(), embedding_model=SBertEmbeddingModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee46f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RA = RetrievalAugmentation(config=RAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfebc24",
   "metadata": {},
   "source": [
    "## Building the tree by loading knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe05daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('demo/sample.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "print(text[:100])    \n",
    "RA.add_documents(text)\n",
    "\n",
    "SAVE_PATH = \"demo/tech_txt_tree_structure\"\n",
    "RA.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e50885d",
   "metadata": {},
   "source": [
    "## Testing the response by asking questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee5847",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is time\"\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "\n",
    "if answer is not None:\n",
    " print(\"Answer: \", answer)\n",
    "else:\n",
    "    print(\"No answer found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff0cf4a",
   "metadata": {},
   "source": [
    "## Visualizing Tree Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5131864",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = RA.tree\n",
    "tree.root_nodes\n",
    "def print_tree_layers(root_nodes):\n",
    "    \"\"\"\n",
    "    Iterates over the tree from the root nodes and prints node index and text layer by layer.\n",
    "\n",
    "    Args:\n",
    "      root_nodes: A dictionary mapping node index to Node objects.\n",
    "    \"\"\"\n",
    "        \n",
    "    all_nodes = tree.all_nodes\n",
    "    current_layer = list(root_nodes.values())  # Convert root_nodes to a list for iteration\n",
    "    level = 0\n",
    "    while current_layer:\n",
    "        print(f\"================= Level {level} ================= \")\n",
    "        next_layer = []\n",
    "        for node in current_layer:\n",
    "            print(f\"Index: {node.index}, Text: {node.text}\\n\")\n",
    "            next_layer.extend(all_nodes.get(child_index) for child_index in node.children)\n",
    "        \n",
    "        current_layer = next_layer\n",
    "        level += 1\n",
    "\n",
    "print_tree_layers(tree.root_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338b2d4",
   "metadata": {},
   "source": [
    "## Loading the saved tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca5f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"demo/sample_txt_tree_structure\"\n",
    "RA = RetrievalAugmentation(config=RAC,tree=SAVE_PATH)\n",
    "question = \"how is the time cindrella living\"\n",
    "answer = RA.answer_question(question=question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
